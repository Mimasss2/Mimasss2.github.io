<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>无监督句子多样性评价指标</title>
    <link href="/2023/01/21/NLP_sentence_diversity_evaluation/"/>
    <url>/2023/01/21/NLP_sentence_diversity_evaluation/</url>
    
    <content type="html"><![CDATA[<blockquote><p>在项目推进过程中，产生了对生成句子多样性进行评价、筛选的需求。遂调研了部分现有的无监督句子多样性的评价指标，以备参考使用</p></blockquote><h2 id="bertscore">BERTScore</h2><p>paper: <a href="https://arxiv.org/abs/1904.09675v3">BERTSCORE: EVALUATING TEXT GENERATION WITH BERT</a></p><p><img src="/img/nlp_sentence_diversity/bertscore.png" /></p><p>每个词找另一个句子中和它内积最大的词</p><p><span class="math display">\[R_{BERT} = \frac{1}{|x|} \underset{x_i \in x}{\Sigma} \underset{\hat{x}\in \hat{x}} {max}  x_i^{T} \hat{x_j},\quadP_{BERT} = \frac{1}{\|\hat{x}\|} \underset{\hat{x_i} \in \hat{x}}{\Sigma} \underset{\hat{x} \in \hat{x}}{max} x_i^{T} \hat{x_j}, \quadF_{BERT} = 2\frac{P_{BERT}\cdot R_{BERT}}{P_{BERT} + R_{BERT}}\]</span></p><h4 id="importance-weighting">Importance Weighting</h4><p>based on inverse document frequency</p><p><span class="math display">\[idf(w) = -\log \frac{1}{M} \Sigma_{i=1}^{M} I [w \in x^{(i)}]\]</span></p><h4 id="rescaling">rescaling</h4><p><span class="math display">\[\hat{R}_{BERT} = \frac{R_{BERT} - b}{1-b}\]</span></p><p>b: empirical lower bound, calculated using Common Crawl monolingual datasets</p><h4 id="comparison">Comparison</h4><p>machine translation evalution -&gt; <span class="math inline">\(F_{BERT}\)</span></p><p>text generation in Eglish -&gt; 24-layer <span class="math inline">\(RoBERTa_{large}\)</span></p><p>non-English language -&gt; <span class="math inline">\(BERT_{multi}\)</span></p><h2 id="bleurt">BLEURT</h2><p>paper: <a href="https://arxiv.org/abs/2004.04696">BLEURT: Learning Robust Metrics for Text Generation. ACL 2020</a></p><h4 id="architecture">Architecture</h4><p>Bert + Linear Head</p><h4 id="pre-training-scheme">pre-training scheme</h4><p>random perturbations of Wikipedia sentences augmented with a diverse set of lexical and semantic-level supervision signals</p><ul><li>mask-filling with BERT -&gt; lexical alterations</li><li>backtranslation</li><li>randomly dropping out words -&gt; to recognize void preditions and sentence truncation in NLG systems</li></ul><p>pretraining metrics: weighted sum of previous metrics</p><h3 id="bartscore">BARTScore</h3><p>paper: <a href="https://arxiv.org/abs/2106.11520">BARTSCORE: Evaluating Generated Text as Text Generation</a></p><p>ExplainaBoard：http://explainaboard.nlpedia.ai/leaderboard/task-meval/</p><figure><img src="/img/nlp_sentence_diversity/bart_board.png" alt="image-20221128102518705" /><figcaption aria-hidden="true">image-20221128102518705</figcaption></figure><h4 id="evaluation-perspectives">evaluation perspectives:</h4><ul><li>Informativeness</li><li>Relevance</li><li>Fluency</li><li>Coherence</li><li>FActuality</li><li>Semantic Coverage</li><li>Adequacy</li></ul><h4 id="bartscore-1">BARTScore</h4><p><span class="math display">\[BARTSCORE = \Sigma_{t=1}^{m} \omega_t \log p(y_t | y_{&lt;t}, x, \theta)\]</span></p><p>using prompt to augment metrics</p><p>没太看明白，一开始列了一堆指标，最后又只有一个BARTScore。看了眼ExplainaBoard，猜测可能是评判的任务/输入数据对<span class="math inline">\(\{x,y\}\)</span>不同，BARTScore体现出的评判句子的方面就不一样</p><h2 id="moverscore">MoverScore</h2><p>MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance. <a href="https://arxiv.org/abs/1909.02622">link</a></p><h4 id="moverdistance">MoverDistance</h4><p><span class="math display">\[WMD(x^n, y^n) := \underset{F \in R^{|x^n| \times |y^n|}}{min} &lt;C,F&gt;,\quad s.t. F1 = f_{x^n}, F^T 1 = f_{y^n}\]</span></p><p><span class="math inline">\(C_{ij} = d(x_i^n, y_j^n)\)</span>, the distance between the i-th n-gram of x and the j-th n-gram of y</p><p><span class="math inline">\(F\)</span>: transportation flow matrix, <span class="math inline">\(F_{ij}\)</span> denoting the amount of flow traveling from the ith n-gram <span class="math inline">\(x_i^n\)</span> in <span class="math inline">\(x^n\)</span> to the j-th n-gram <span class="math inline">\(y_j^n\)</span> in <span class="math inline">\(y^n\)</span>.</p><p><span class="math inline">\(&lt;C,F&gt; = sum(C \odot F)\)</span></p><p><span class="math inline">\(d(x_i^n, y_j^n)\)</span> Euclidean distance</p><p><span class="math inline">\(f_{x^n_i} = \frac{1}{Z} \Sigma_{k=i}^{i+n-1} idf(x_k)\)</span></p><h4 id="vs-bertscore">vs BERTScore</h4><figure><img src="/img/nlp_sentence_diversity/bartscore_vs_bertscore.png" alt="image-20221128093223272" /><figcaption aria-hidden="true">image-20221128093223272</figcaption></figure><p>对于某个词，BERTScore算原句子中与它最相似的词的相似度（内积），而MoverScore算这个词和所有其他词的加权内积和，权重（即公式中的<span class="math inline">\(F\)</span>）通过idx算</p><h2 id="embedding-average">Embedding Average</h2><p>直接计算生成文本和参考文本中词向量的平均值作为文本的向量表示，然后计算两个文本的余弦相似度作为生成文本和参考文本的相似度：</p><p><span class="math display">\[\bar{e_r} = \frac{\Sigma_{\omega \in r} e_{\omega}}{| \Sigma_{\omega &#39; \in r} e_{\omega &#39;}|}\]</span> <span class="math display">\[EA := cos(\bar{e_r}, \bar{e_{\hat{r}}})\]</span></p><h2 id="perplexity">Perplexity</h2><h2 id="p.s.">p.s.</h2><p>很多现有的评价生成的文本的指标是基于Machine Translation任务的，计算原句子和翻译句子的相似度/匹配度。</p><h5 id="评价句子相似度的一些指标">评价句子相似度的一些指标</h5><ul><li><a href="https://aclanthology.org/S17-2001/">Semantic Textual Similarity</a></li><li><a href="https://aclanthology.org/W17-4767/">MEANT 2.0</a></li><li><a href="https://arxiv.org/abs/2109.06379">Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation</a></li><li>…</li></ul>]]></content>
    
    
    <categories>
      
      <category>NLP</category>
      
    </categories>
    
    
    <tags>
      
      <tag>NLP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>git cheatsheet</title>
    <link href="/2022/12/09/git_cheatsheet/"/>
    <url>/2022/12/09/git_cheatsheet/</url>
    
    <content type="html"><![CDATA[<p>A git command cheatsheet.</p><h4 id="create-repo">1 create repo</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cli">git init<br></code></pre></td></tr></table></figure><p>初始化当前目录为git仓库</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cli">git clone &lt;url&gt; <br>git clone &lt;url&gt; &lt;dir&gt; # 将仓库克隆到指定的目录<br></code></pre></td></tr></table></figure><p>从<code>url</code>克隆仓库</p><h4 id="modify-and-commit">2 modify and commit</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cli">git add &lt;filename&gt; # 添加名为filename的文件的变化<br>git add . # 添加当前目录下及其子目录下的所有文件的变化<br></code></pre></td></tr></table></figure><p>添加文件到暂存区</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cli">git commit -m &lt;version comment&gt; # 将当前版本提交，并添加注释`&lt;version comment&gt;`<br></code></pre></td></tr></table></figure><p>提交暂存区到仓库。</p>]]></content>
    
    
    <categories>
      
      <category>Cheatsheet</category>
      
    </categories>
    
    
    <tags>
      
      <tag>git</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>my new post</title>
    <link href="/2022/12/09/my-new-post/"/>
    <url>/2022/12/09/my-new-post/</url>
    
    <content type="html"><![CDATA[<h2 id="my-new-post">My new post</h2><p>post test2.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/12/08/hello-world/"/>
    <url>/2022/12/08/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
